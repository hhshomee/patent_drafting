{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyND7DNVw26SVsWwchh1pxqb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0czQjXarUiet"},"outputs":[],"source":["!pip install bert_score"]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"_Qb5gRVVUq9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv('/content/GeneratedAbstract_A61B.csv')"],"metadata":{"id":"85cbVa5vUs7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from bert_score import score\n","\n","def calculate_bertscore(candidates, references, lang='en', verbose=False):\n","    \"\"\"\n","    Calculate BERTScore for lists of candidate and reference sentences.\n","\n","    Args:\n","        candidates: a list of candidate sentences (strings)\n","        references: a list of reference sentences (strings)\n","        lang: language of the sentences (default is English)\n","        verbose: verbosity of BERTScore (default is False)\n","\n","    Returns:\n","        A tuple of three lists containing precision, recall, and F1 scores for each candidate-reference pair\n","    \"\"\"\n","    P, R, F1 = score(candidates, references, lang=lang, verbose=verbose)\n","    return P, R, F1\n","\n","\n","candidates = df['generated_abstract'].astype(str).tolist()\n","references = df['patent_abstract'].astype(str).tolist()\n","\n","\n","P, R, F1 = calculate_bertscore(candidates, references)\n","df.loc[:, 'bert_Precision'] = P\n","df.loc[:, 'bert_Recall'] = R\n","df.loc[:, 'bert_F1_Score'] = F1\n","\n","df.to_csv('Evaluation_abstract_A61B_bert.csv', index=False)\n"],"metadata":{"id":"0qF7puIkUz51"},"execution_count":null,"outputs":[]}]}