{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9qJdHFyeD4nWK8D+xBw47"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","df=pd.read_csv('/content/GeneratedAbstract_A61B.csv')"],"metadata":{"id":"0QGA-tnuVg8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrZNYTVSVVLQ"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","\n","def calculate_bleu_score(reference_text, generated_text):\n","    \"\"\"\n","    Calculate BLEU score for a reference and a generated text.\n","\n","    Args:\n","        reference_text (str): The reference text.\n","        generated_text (str): The generated text by LLM.\n","\n","    Returns:\n","        float: The BLEU score.\n","    \"\"\"\n","    a=[]\n","    for i in range (0,len(reference_text)):\n","      reference_tokens = reference_text[i].split()\n","      generated_tokens = generated_text[i].split()\n","      cc = SmoothingFunction()\n","      score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=cc.method1)\n","      a.append(score)\n","    return a\n","\n","gen = df['generated_abstract'].astype(str).tolist()\n","ref = df['patent_abstract'].astype(str).tolist()\n","\n","bleu_score = calculate_bleu_score(ref, gen)\n","df.loc[:, 'bleu_score'] = [float(f\"{score:.2f}\") for score in bleu_score]\n","\n","df.to_csv('Evaluation_abstract_A61B_bleu.csv', index=False)"]}]}