{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63232,"status":"ok","timestamp":1712025581411,"user":{"displayName":"Homaira Shomee","userId":"10988384891589437600"},"user_tz":300},"id":"PXhDGu4sq-S_","outputId":"3e86553d-a678-45c1-9e32-36cac1af6091"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q git+https://github.com/huggingface/transformers.git\n","!pip install -q tensorflow==2.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfZjaX51rCpS"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDvScohBswS2"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('/content/patent_data_A61B.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Q8SLtsHqtYX-"},"outputs":[],"source":["df['generated_abstract'] = pd.NA\n","def generate_abstract(claim_text):\n","    try:\n","        model_inputs = tokenizer(f'this is a patent claim:{claim_text}. Generate the abstract for this claim',return_tensors='pt').to(torch_device)\n","        sample_output = model.generate(\n","                        **model_inputs,\n","                        do_sample=True,\n","                        max_length=500,\n","                        top_k=40\n","                    )\n","        decoded_output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n","        input_prompt = tokenizer.decode(model_inputs['input_ids'][0], skip_special_tokens=True)\n","        cleaned_output = decoded_output[len(input_prompt):] if decoded_output.startswith(input_prompt) else decoded_output\n","\n","\n","        return cleaned_output\n","    except Exception as e:\n","        print(\"An error occurred:\", e)\n","        return None\n","\n","df.iloc[0:0].to_csv('GPT2_A61B.csv', index=False)\n","\n","for index, row in df.iterrows():\n","    print(index)\n","    generated_abstract = generate_abstract(row['claim_text'])\n","    df.at[index, 'generated_abstract'] = generated_abstract if isinstance(generated_abstract, str) else 'Error or empty'\n","    df.iloc[[df.index.get_loc(index)]].to_csv('GPT2_A61B.csv', mode='a', header=False, index=False)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJa6FPvE5AvoVlMTlnjy/o"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}